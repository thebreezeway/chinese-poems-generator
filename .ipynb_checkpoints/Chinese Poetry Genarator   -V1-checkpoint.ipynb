{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Poetry Genarator - V1 -With RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba as jb\n",
    "import numpy as np\n",
    "# from rnn_utils import *\n",
    "from nlp_utils import load_char_embeddings, load_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient,-maxValue,maxValue,out = gradient)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[str(ix)] for ix in  sample_ix)\n",
    "    # txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "\n",
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    b = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    return a_next, p_t\n",
    "\n",
    "def rnn_step_backward_old(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "#     print(\"dy\",dy.shape)\n",
    "#     print(\"dby\",gradients['dby'].shape)\n",
    "   \n",
    "#     print(\"dWax\",gradients['dWax'].shape) \n",
    "#     print(\"dWaa\",gradients['dWaa'].shape)\n",
    "#     print(\"da_next\",gradients['da_next'].shape)\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    gradients['dWya'] += 1/m*np.sum(np.dot(dy, a.T), axis=1, keepdims=True)\n",
    "    gradients['dby'] += 1/m*np.sum(dy, axis=1, keepdims=True)\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "#     print(\"daraw\", daraw.shape)\n",
    "    gradients['db'] += 1/m*np.sum(daraw, axis=1, keepdims=True)\n",
    "#     print(\"db\",gradients['db'].shape)\n",
    "    gradients['dWax'] += 1/m*np.sum(np.dot(daraw, x.T), axis=1, keepdims=True)\n",
    "    gradients['dWaa'] += 1/m*np.sum(np.dot(daraw, a_prev.T), axis=1, keepdims=True)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    \n",
    "#     print(\"da\",da.shape)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev, beta=0.99):\n",
    "    \n",
    "#     print(\"dy\",dy.shape)\n",
    "#     print(\"dby\",gradients['dby'].shape)\n",
    "   \n",
    "#     print(\"dWax\",gradients['dWax'].shape) \n",
    "#     print(\"dWaa\",gradients['dWaa'].shape)\n",
    "#     print(\"da_next\",gradients['da_next'].shape)\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    gradients['dWya'] += beta*gradients['dWya'] + (1- beta) * 1/m*np.sum(np.dot(dy, a.T), axis=1, keepdims=True)\n",
    "    gradients['dby'] += beta *gradients['dby'] +(1-beta) * 1/m*np.sum(dy, axis=1, keepdims=True)\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "#     print(\"daraw\", daraw.shape)\n",
    "    gradients['db'] += beta *gradients['db'] +(1-beta) * 1/m*np.sum(daraw, axis=1, keepdims=True)\n",
    "#     print(\"db\",gradients['db'].shape)\n",
    "    gradients['dWax'] += beta * gradients['dWax']+(1-beta) * 1/m*np.sum(np.dot(daraw, x.T), axis=1, keepdims=True)\n",
    "    gradients['dWaa'] += beta * gradients['dWaa']+(1-beta) * 1/m*np.sum(np.dot(daraw, a_prev.T), axis=1, keepdims=True)\n",
    "    gradients['da_next'] = beta *  gradients['da_next']+(1-beta) * np.dot(parameters['Waa'].T, daraw)\n",
    "    \n",
    "#     print(\"da\",da.shape)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "    \n",
    "    m = X[0].shape[1]\n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    \n",
    "    a, y_hat = {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    # initialize your loss to 0\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        # x[t] = np.zeros((vocab_size,1)) \n",
    "        # if (X[t] != None):\n",
    "        #     x[t][X[t]] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], X[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        dloss = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            dloss += 1/m* np.log(y_hat[t][Y[t][i],0])\n",
    "        loss -= dloss\n",
    "        \n",
    "    cache = (y_hat, a, X)\n",
    "        \n",
    "    return loss, cache\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    m = x[0].shape[1]\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        for i in range(m):\n",
    "             dy[Y[t][i]][i] -= 1\n",
    "        \n",
    "       \n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index, index_to_char, index_to_vec = load_char_embeddings(\"../../data/embedding/sogou/char_embedding_plus_punctuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_len =  6902 emb_dim =  (300,)\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(char_to_index) + 2\n",
    "emb_dim = index_to_vec['0'].shape\n",
    "print(\"vocab_len = \", vocab_len, \"emb_dim = \", emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding fitures are between -1.685101 1.671867\n",
    "# append EOS and UNK to embedding matrixz\n",
    "np.random.seed(ord('E'))\n",
    "char_to_index['<EOS>'] = 6900\n",
    "index_to_char['6900'] = '<EOS>'\n",
    "index_to_vec['6900'] = np.random.rand(300,)\n",
    "\n",
    "np.random.seed(ord('U'))\n",
    "char_to_index['<UNK>'] = 6901\n",
    "index_to_char['6901'] = '<UNK>'\n",
    "index_to_vec['6901'] = np.random.rand(300,)\n",
    "\n",
    "#define /n as EOS\n",
    "char_to_index['\\n'] = 6900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6902"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangshis = np.load(\"../../data/chinese-poetry-master/tangshi_v1_5yan.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'秦川雄帝宅，函谷壮皇居。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tangshis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset= tangshis\n",
    "# index = 5\n",
    "# batch_size = 4\n",
    "# mini_batch = []\n",
    "# Y = []\n",
    "# for c in range(12):\n",
    "#     char_batch = []\n",
    "#     char_batch_ix = []\n",
    "#     for i in range(batch_size): \n",
    "#         ix = char_to_index['<EOS>'] #初始化而已\n",
    "#         try:\n",
    "#             ix = char_to_index[dataset[index+i][c]]\n",
    "#         except KeyError:\n",
    "#             ix = char_to_index['<UNK>'] \n",
    "#         char_batch_ix.append(ix)\n",
    "#         char_batch.append(index_to_vec[str(ix)])\n",
    "#     Y.append(char_batch_ix)\n",
    "#     mini_batch.append(char_batch)\n",
    "# Y.append([char_to_index['<EOS>']]*4)\n",
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_params(parameters):\n",
    "    np.save(\"saves/parameters_\"+str(parameters['endpoint'])+'.npy', parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, ix_to_char, ix_to_vec, seed, fixed_chars = None, padding = False ):\n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
    "    x = np.zeros((300,1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1\n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugxging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 12):\n",
    "       \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        \n",
    "        a = np.tanh(np.dot( Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter+seed) \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        if fixed_chars!=None and counter<len(fixed_chars):\n",
    "            idx = char_to_ix[fixed_chars[counter]]\n",
    "        else:\n",
    "            while True:\n",
    "                idx = np.random.choice(range(vocab_size),p = y.ravel())\n",
    "                if not padding or idx != char_to_index[\"<EOS>\"]:\n",
    "\n",
    "                    break\n",
    "            \n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = index_to_vec[str(idx)].reshape(300,1)\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(dataset,index_to_char, char_to_index, index_to_vec, num_iterations = 120000, n_a = 64,gen_samples = 5,parameters = None, batch_size = 64,learning_rate= 0.01):\n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = 300, len(char_to_index)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    if not parameters:\n",
    "        parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "        parameters['endpoint'] = 0\n",
    "        parameters['time'] = []\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    # loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, batch_size))\n",
    "    j = 0\n",
    "    import time\n",
    "    time_start=time.time()\n",
    "    epoch_size = len(dataset)\n",
    "    try:\n",
    "    # Optimization loop\n",
    "        for j in range(parameters['endpoint'],parameters['endpoint'] + num_iterations):\n",
    "\n",
    "            ### START CODE HERE ###\n",
    "\n",
    "            # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "            index = j*batch_size%len(dataset)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            X = []\n",
    "            Y = []\n",
    "            \n",
    "            # 讲汉字转换为向量输入和索引输出，并处理未知字符。生成输入矩阵\n",
    "            X.append(np.zeros((300,batch_size)))\n",
    "            for c in range(12):\n",
    "                char_batch = []\n",
    "                char_batch_ix = []\n",
    "                for i in range(batch_size): \n",
    "                    ix = char_to_index['<EOS>'] #初始化而已\n",
    "                    try:\n",
    "                        ix = char_to_index[dataset[index+i][c]]\n",
    "                    except KeyError:\n",
    "                        ix = char_to_index['<UNK>'] \n",
    "                        \n",
    "                    char_batch_ix.append(ix)\n",
    "                    char_batch.append(index_to_vec[str(ix)])\n",
    "                    \n",
    "                X.append(np.array(char_batch).T)\n",
    "                Y.append(char_batch_ix)\n",
    "            \n",
    "            Y.append([char_to_index[\"<EOS>\"]]*batch_size)\n",
    "\n",
    "            \n",
    "\n",
    "            # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "            # Choose a learning rate of 0.01\n",
    "            loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = learning_rate)\n",
    "\n",
    "            # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "            # loss = smooth(loss, curr_loss)\n",
    "\n",
    "            # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "            \n",
    "            print(\"进度:{}/{} epoches\".format(j*batch_size,epoch_size), end=\"\\r\")\n",
    "             \n",
    "            if j*batch_size % 512 == 0:\n",
    "                print(\"\\n耗时\",round(time.time()-time_start,3),\"s\")\n",
    "                print('Iteration: %d, Loss: %7f' % (j*batch_size, loss) + '\\n')\n",
    "\n",
    "                # The number of dinosaur names to print\n",
    "                seed = 0\n",
    "                for s in range(gen_samples):\n",
    "                    \n",
    "                    # Sample indices and print them\n",
    "                    sampled_indices = sample(parameters, char_to_index, index_to_char, index_to_vec, seed)\n",
    "                    print_sample(sampled_indices, index_to_char)\n",
    "                    print('')\n",
    "                    seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "\n",
    "                print('\\n')\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        \n",
    "        parameters['endpoint'] = j\n",
    "        time_end=time.time()\n",
    "        parameters['time'].append(round(time_end-time_start,3))\n",
    "        save_params(parameters)\n",
    "         \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " parameters= np.load(\"saves/parameters_190454.npy\")[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters['time'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进度:97512448/154162 epoches\n",
      "耗时 1.859 s\n",
      "Iteration: 97512448, Loss: 75.148441\n",
      "\n",
      "兴住朽旌浑，因今小帝仙。\n",
      "旧兵无不在，浩荡无人地。\n",
      "昨烛尘蓬潞，落金烟隔深。\n",
      "形应日为者，徒此从不终。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97512960/154162 epoches\n",
      "耗时 3.815 s\n",
      "Iteration: 97512960, Loss: 75.225317\n",
      "\n",
      "兴住忻夷，疾去平生。<EOS>\n",
      "旧兵无不在，浩荡长成分。\n",
      "阿勘悲幽揫，乡去皆归期。\n",
      "形自后为此，闲作不有疑。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97513472/154162 epoches\n",
      "耗时 5.659 s\n",
      "Iteration: 97513472, Loss: 74.049158\n",
      "\n",
      "兴海嘶壑，启所拟问。<EOS>\n",
      "旧兵无不在，浩荡无人地。\n",
      "昨侍悲幽舜，何由寺田谁。\n",
      "形自后为此，闲作不有疑。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97513984/154162 epoches\n",
      "耗时 7.79 s\n",
      "Iteration: 97513984, Loss: 74.331152\n",
      "\n",
      "兴住忻袍绮，应叶带池萍。\n",
      "旧徐如有一，虚隐受地三。\n",
      "阿敛辨鹿瑟，断电遍烟园。\n",
      "形自后为老，闲生不有语。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97514496/154162 epoches\n",
      "耗时 10.193 s\n",
      "Iteration: 97514496, Loss: 76.141449\n",
      "\n",
      "兴投谅钓宦，所合共逢妆。\n",
      "旧兵无不在，始验作分名。\n",
      "阿甸卿衔涩，复自逢云药。\n",
      "形自他为此，闲作不有疑。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97515008/154162 epoches\n",
      "耗时 12.264 s\n",
      "Iteration: 97515008, Loss: 76.183639\n",
      "\n",
      "兴拟诏卒谬，所招近汉仁。\n",
      "旧兵无不在，始欢发行时。\n",
      "昨贻辞雁浦，似此知载山。\n",
      "形五日将图，闲方不对营。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97515520/154162 epoches\n",
      "耗时 14.368 s\n",
      "Iteration: 97515520, Loss: 76.763320\n",
      "\n",
      "兴德夙蚌槿，几马只何言。\n",
      "旧钟近月在，尝晓自我多。\n",
      "阿鲤尘枕隐，彩景雪额节。\n",
      "形自就一均，倚岛月中落。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97516032/154162 epoches\n",
      "耗时 16.756 s\n",
      "Iteration: 97516032, Loss: 75.260083\n",
      "\n",
      "兴德幔苍砥，开床起兰奔。\n",
      "旧儿如不在，益昔未已三。\n",
      "阿鲤尘枕隐，彩景雪朱花。\n",
      "形自不为此，堪生不有论。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97516544/154162 epoches\n",
      "耗时 19.143 s\n",
      "Iteration: 97516544, Loss: 75.842222\n",
      "\n",
      "兴拟诏隼豳，高火系储缝。\n",
      "旧儿如不在，勿羡难我还。\n",
      "阿敛择纤洽，何由依凡道。\n",
      "形自不为此，逆生不有语。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97517056/154162 epoches\n",
      "耗时 21.276 s\n",
      "Iteration: 97517056, Loss: 74.161896\n",
      "\n",
      "兴德鄱窦僻，高石想似闻。\n",
      "旧儿如不在，勿羡难及之。\n",
      "阿敛哀仲麻，似令射琴林。\n",
      "形自不为此，烦话从不敢。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97517568/154162 epoches\n",
      "耗时 23.479 s\n",
      "Iteration: 97517568, Loss: 74.513090\n",
      "\n",
      "兴德樵吟，幸之降声。<EOS>\n",
      "旧儿近不在，尝凶且大来。\n",
      "阿敛哀释隐，乐门复何像。\n",
      "形自不为此，皇花不有论。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97518080/154162 epoches\n",
      "耗时 26.021 s\n",
      "Iteration: 97518080, Loss: 74.400409\n",
      "\n",
      "兴德藜辇眺，曾微当归寻。\n",
      "旧夫无人在，尺禹非我多。\n",
      "昨朽俱昭详，歌声护认平。\n",
      "形自不为此，皇花不有优。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97518592/154162 epoches\n",
      "耗时 28.439 s\n",
      "Iteration: 97518592, Loss: 75.084526\n",
      "\n",
      "兴拟嘶樽，紫若定晚。<EOS>\n",
      "旧儿近不在，紫冕自能来。\n",
      "昨侍悲幽喻，春开舞鼓声。\n",
      "邀得还一下，鹅马不有修。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97519104/154162 epoches\n",
      "耗时 30.907 s\n",
      "Iteration: 97519104, Loss: 74.943778\n",
      "\n",
      "兴德樵吟，杂即逾重。<EOS>\n",
      "旧豪期个在，目矣非要及。\n",
      "昨筝悲冶垂，田马盛青林。\n",
      "尔于时一去，犹令而有意。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97519616/154162 epoches\n",
      "耗时 33.4 s\n",
      "Iteration: 97519616, Loss: 74.324348\n",
      "\n",
      "兴德浣蛾冽，数险自闻申。\n",
      "旧儿女地中，紫箔变四看。\n",
      "昨碣悲赋狐，留才争复回。\n",
      "尔于时一去，犹令而有意。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97520128/154162 epoches\n",
      "耗时 35.65 s\n",
      "Iteration: 97520128, Loss: 75.907314\n",
      "\n",
      "兴德膊缨舃，高流水墙斜。\n",
      "旧儿女说一，召卜未能能。\n",
      "昨曙堪烦贱，英马复何车。\n",
      "尔于时一去，犹令上人梦。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97520640/154162 epoches\n",
      "耗时 37.7 s\n",
      "Iteration: 97520640, Loss: 75.468948\n",
      "\n",
      "兴德樵羞敛，因别近肝黎。\n",
      "旧儿共有一，瀑奴自由之。\n",
      "昨曙堪堪叹，春没公盈持。\n",
      "尔于时一去，犹令而有意。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97521152/154162 epoches\n",
      "耗时 39.57 s\n",
      "Iteration: 97521152, Loss: 76.293136\n",
      "\n",
      "兴德樵鳞掇，得尽起续亲。\n",
      "旧兵无不在，霞辱无不多。\n",
      "昨苍怜徒贱，露深境彩光。\n",
      "尔新时一好，堪作月中精。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97521664/154162 epoches\n",
      "耗时 41.582 s\n",
      "Iteration: 97521664, Loss: 74.929345\n",
      "\n",
      "兴德浣弥澹，看清五塞衣。\n",
      "旧儿共有为，始忍事不多。\n",
      "昨曙垂覆舌，依回烧庭园。\n",
      "尔新时一好，介远不从苏。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97522176/154162 epoches\n",
      "耗时 43.499 s\n",
      "Iteration: 97522176, Loss: 74.813176\n",
      "\n",
      "兴德膊躯僻，只病如青池。\n",
      "旧儿共有一，洒妾自三时。\n",
      "昨烛宿锆靦，落路落叶花。\n",
      "尔新时一种，堪作月中复。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97522688/154162 epoches\n",
      "耗时 45.33 s\n",
      "Iteration: 97522688, Loss: 74.215717\n",
      "\n",
      "兴德恢阙樵，高乘近玉池。\n",
      "旧儿共有一，飘袍正三月。\n",
      "昨笠历栖穴，临令复何过。\n",
      "尔新时一使，孰令不不知。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97523200/154162 epoches\n",
      "耗时 47.233 s\n",
      "Iteration: 97523200, Loss: 74.615661\n",
      "\n",
      "兴德夙吟戢，成儿看落薪。\n",
      "旧夫无人在，溪舟老月地。\n",
      "昨烛窥宾丘，临入曲草间。\n",
      "尔前月为开，疏师月年春。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97523712/154162 epoches\n",
      "耗时 49.059 s\n",
      "Iteration: 97523712, Loss: 75.661878\n",
      "\n",
      "兴住侍幽秃，高峰半角蛇。\n",
      "旧夫无人在，溪舟老月多。\n",
      "昨烛窥宾履，留去愿利神。\n",
      "尔前月为开，疏题月中形。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97524224/154162 epoches\n",
      "耗时 50.897 s\n",
      "Iteration: 97524224, Loss: 74.525915\n",
      "\n",
      "礼听幷崇珀，得推如断碟。\n",
      "旧夫无人在，溪舟先不名。\n",
      "昨烛宿茎甬，断海势何发。\n",
      "尔前月为数，辞胜都不敢。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97524736/154162 epoches\n",
      "耗时 52.815 s\n",
      "Iteration: 97524736, Loss: 75.135387\n",
      "\n",
      "礼镇樵僧嗣，高居约席驰。\n",
      "旧夫无人在，溪舟谁时多。\n",
      "昨辙丧悠悠，流金荷御除。\n",
      "尔前月为者，衰心不有语。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97525248/154162 epoches\n",
      "耗时 54.677 s\n",
      "Iteration: 97525248, Loss: 73.987363\n",
      "\n",
      "礼变蟠阶僻，高坐好章梅。\n",
      "旧夫无人在，溪舟全下前。\n",
      "昨碣尘驭掷，杀高雪吹声。\n",
      "尔前月为者，饱含月中农。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97525760/154162 epoches\n",
      "耗时 56.971 s\n",
      "Iteration: 97525760, Loss: 74.163844\n",
      "\n",
      "礼海棱啾杵，太愿起君终。\n",
      "旧微开其在，扶邑非不地。\n",
      "昨磬碧鞍织，落处荷租花。\n",
      "哭看到将处，渔住月中烟。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97526272/154162 epoches\n",
      "耗时 59.529 s\n",
      "Iteration: 97526272, Loss: 74.910685\n",
      "\n",
      "礼变谕筵蔑，车马四青阳。\n",
      "旧辟余大也，尝欢万里还。\n",
      "昨笠历翅穴，丝室知归道。\n",
      "哭看从一家，玄山不不营。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97526784/154162 epoches\n",
      "耗时 61.862 s\n",
      "Iteration: 97526784, Loss: 74.163850\n",
      "\n",
      "礼学萦饥噪，应稳自判亡。\n",
      "旧调此日在，虚隐事人多。\n",
      "昨竭芸樽柄，复如汉良收。\n",
      "哭看从一出，闲心月中惊。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97527296/154162 epoches\n",
      "耗时 64.29 s\n",
      "Iteration: 97527296, Loss: 74.073160\n",
      "\n",
      "礼学牲奠泮，因风无弱疆。\n",
      "旧旧长能在，私束长不还。\n",
      "昨磬嵌猿屿，枪马抵床林。\n",
      "兴天时一去，闲心还中疑。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97527808/154162 epoches\n",
      "耗时 67.502 s\n",
      "Iteration: 97527808, Loss: 75.758351\n",
      "\n",
      "礼话寝旋憩，看唱发附曲。\n",
      "旧旧长之在，溪舟度下来。\n",
      "昨曙倚栖穴，翻马射威根。\n",
      "兴天以一处，窜白不人求。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97528320/154162 epoches\n",
      "耗时 69.955 s\n",
      "Iteration: 97528320, Loss: 76.060255\n",
      "\n",
      "扣石窘蹄缁，发杀此何亲。\n",
      "旧调此日在，勿劳千不还。\n",
      "昨谏徒舜绩，何由谢止南。\n",
      "兴出多一去，淹晚月中冰。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97528832/154162 epoches\n",
      "耗时 72.688 s\n",
      "Iteration: 97528832, Loss: 76.106347\n",
      "\n",
      "伍游蒿虏碍，只感开牙雕。\n",
      "英玉应不是，俗羡八月人。\n",
      "昨沓惜徒猎，何由复求通。\n",
      "兴据从一所，闲见月中归。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97529344/154162 epoches\n",
      "耗时 74.949 s\n",
      "Iteration: 97529344, Loss: 74.149597\n",
      "\n",
      "望往涓筵，帐去照门。<EOS>\n",
      "英语才不在，虚闲长已多。\n",
      "昨烛悲蜀隐，调花烟谷连。\n",
      "兴天以一处，翘广会中词。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97529856/154162 epoches\n",
      "耗时 78.101 s\n",
      "Iteration: 97529856, Loss: 73.623503\n",
      "\n",
      "望往嘶蜕鬓，开树无雪鸟。\n",
      "英语张都在，珍髻若我还。\n",
      "昨锄覆讽簇，翻通散摇杨。\n",
      "兴天时一去，闲心但有意。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97530368/154162 epoches\n",
      "耗时 81.684 s\n",
      "Iteration: 97530368, Loss: 75.329882\n",
      "\n",
      "望往掖胤砣，应亚共痛秋。\n",
      "英语张都在，莲菊送更新。\n",
      "昨侍窥鲸啼，沙水惊珠沙。\n",
      "兴比月与余，昭山还有归。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97530880/154162 epoches\n",
      "耗时 87.611 s\n",
      "Iteration: 97530880, Loss: 74.169418\n",
      "\n",
      "望往嘶啼翰，几重共苑滨。\n",
      "英汉当上中，怀朋自我之。\n",
      "昨辇萧萧楚，叶声遂归长。\n",
      "兴天及一过，忍节不不知。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97531392/154162 epoches\n",
      "耗时 90.845 s\n",
      "Iteration: 97531392, Loss: 74.213312\n",
      "\n",
      "望往萤霄烛，无班像帝器。\n",
      "英玉应日上，危闰红红发。\n",
      "昨烛牧陀哀，何由复宜平。\n",
      "兴天时一去，闲心但有意。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97531904/154162 epoches\n",
      "耗时 94.133 s\n",
      "Iteration: 97531904, Loss: 75.447889\n",
      "\n",
      "望往践悲吟，高明应复闻。\n",
      "英玉应日上，帆邑自由之。\n",
      "昨遁慈戚欢，辟路意乐轮。\n",
      "兴天时是使，悲远从上冰。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97532416/154162 epoches\n",
      "耗时 96.583 s\n",
      "Iteration: 97532416, Loss: 73.851100\n",
      "\n",
      "望往诏栖梗，十重共兴阳。\n",
      "铜影发分后，舟睛亦不多。\n",
      "昨溦覆隼沥，争节舞琴黄。\n",
      "兴出多一家，犹期日有春。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97532928/154162 epoches\n",
      "耗时 99.018 s\n",
      "Iteration: 97532928, Loss: 75.630054\n",
      "\n",
      "望往践悲咏，几通自复顾。\n",
      "铜影发则在，枝阔见新来。\n",
      "昨罄幽霄际，助余玉署生。\n",
      "兴天时是使，悲远从上冰。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97533440/154162 epoches\n",
      "耗时 101.098 s\n",
      "Iteration: 97533440, Loss: 75.030328\n",
      "\n",
      "望往雏翎襟，高明应复闻。\n",
      "铜影发分后，舟啸入四去。\n",
      "露峤腻怯昏，吹杨鲜豆牛。\n",
      "兴天时是使，悲远从上草。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97533952/154162 epoches\n",
      "耗时 103.729 s\n",
      "Iteration: 97533952, Loss: 75.613236\n",
      "\n",
      "望往嘶呜，隐处清风。<EOS>\n",
      "虎腰台向对，怀孩无人并。\n",
      "露膻咽霓羽，潮水势微风。\n",
      "兴天时是使，悲远从上观。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97534464/154162 epoches\n",
      "耗时 105.962 s\n",
      "Iteration: 97534464, Loss: 75.711865\n",
      "\n",
      "望若矩粟狎，高极使善幼。\n",
      "铜宫无不在，击迹每时时。\n",
      "露檮杏炊邑，似作曲帝方。\n",
      "兴因从一处，犹开日中贵。\n",
      "<EOS>\n",
      "\n",
      "\n",
      "进度:97534976/154162 epoches\n",
      "耗时 108.134 s\n",
      "Iteration: 97534976, Loss: 76.028102\n",
      "\n",
      "望若愧幽僻，因风无舰催。\n",
      "铜宫无不在，凤阜占行及。\n",
      "露醅幽昼寒，公文尾滑林。\n",
      "兴请月将好，闲方月上雷。\n",
      "<EOS>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters=model(tangshis, index_to_char, char_to_index, index_to_vec, num_iterations = 10000,batch_size=512,parameters=parameters, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春冀近推好，日自何由流。\n",
      "\n",
      "曙星。<EOS>\n",
      "\n",
      "闰来心名六三卷浮。<EOS>\n",
      "\n",
      "青素。<EOS>\n",
      "\n",
      "此道上华日，翻管许闲漫。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成 诗句\n",
    "seed = 330\n",
    "for name in range(5):\n",
    "                \n",
    "    # Sample indices and print them\n",
    "    sampled_indices = sample(parameters, char_to_index, index_to_char, index_to_vec, seed, fixed_chars=\"\", padding = False)\n",
    "    print_sample(sampled_indices, index_to_char)\n",
    "    seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
